
<img src="https://images.steamusercontent.com/ugc/831387554223571248/68C537E301D110B6664186D4113E23E7E82C5A0B/?imw=637&imh=358&ima=fit&impolicy=Letterbox&imcolor=%23000000&letterbox=true" width="100%"/>
<div align="center">
  
# ArcanAI
### **Private, offline, on-device LLM for iOS.**  

  </div>
<img align = "center" alt="divider" width="9000" src="https://images.hive.blog/DQmPTLhVNyDnLA1pK3vUcmhgCVCDtvqDiE2TE95zU7V5w5Y/hive%20dividers-11.png">


ArcanAI lets you run open-source Ollama models **entirely on your iPhone or iPad** â€” no internet, no accounts, no cloud. Download once, use forever.

## Features
- Zero sign-in â€” no email, no tracking, no telemetry
- 100% offline after model download
- Pre-download model selector â€” choose Phi-3, Mistral, Llama 3.1, Gemma 2 (Q4/Q5)
- On-device inference via llama.cpp with Metal acceleration (20â€“35 tokens/sec on iPhone 15 Pro)
- ChatGPT-style UI with streaming responses and conversation history
- Markdown rendering â€” code blocks, bold, lists, and formatting in responses
- Smart token filtering â€” clean output without model artifacts
- Auto-scroll â€” chat follows responses as they generate
- Stop control â€” halt generation instantly at any time
- Enter to send â€” natural keyboard messaging experience

## Models (Pre-converted `.mlpackage`)
| Model | Size | Params | Use Case |
|-------|------|--------|----------|
| **Phi-3 Mini 4K (Q4)** | 2.3 GB | 3.8B | Fast, general *(bundled)* |
| **Mistral 7B Instruct (Q4)** | 4.1 GB | 7B | Reasoning |
| **Llama 3.1 8B (Q4)** | 4.7 GB | 8B | Coding |
| **Gemma 2 2B (Q5)** | 1.6 GB | 2B | Lightweight |

## Requirements
- iOS 18+
- Apple Silicon (A17 Pro / M2 or later recommended)
- 3â€“6 GB free storage per model

## Privacy
- **No data collected** (App Store Privacy Report)
- No crash reporting
- No network calls after setup

---

## Recent Updates

### ðŸŽ‰ Today's Major Update (2025-11-19)

**Core Functionality Completed**
- âœ… **Real AI Inference** â€” Completed full llama.cpp integration with actual on-device LLM generation (no more mock responses!)
- âœ… **Multi-Turn Conversations** â€” Fixed context management for proper conversation history
- âœ… **Model-Specific Templates** â€” Added proper chat templates for Llama 3.1, Mistral 7B, Phi-3, and Gemma 2
- âœ… **Smart Token Filtering** â€” Intelligent buffering removes special tokens (`<start_of_turn>`, `<end_of_turn>`, etc.) for clean output
- âœ… **Thread-Safe Architecture** â€” Actor-based LlamaContext for safe concurrent access

**User Experience Enhancements**
- âœ… **Markdown Rendering** â€” Rich text formatting with code blocks, bold, italics, lists, and headers
- âœ… **Auto-Scroll** â€” Chat automatically follows AI responses as they stream
- âœ… **Loading States** â€” "Generating..." indicator inside chat bubble before first token
- âœ… **Stop Button** â€” Immediately halt generation mid-response (with proper cleanup)
- âœ… **Enter to Send** â€” Press Enter to submit messages naturally
- âœ… **Text Selection** â€” Copy and paste AI responses

**Technical Improvements**
- âœ… **Proper Task Cancellation** â€” Stop button actually stops llama.cpp inference (prevents crashes)
- âœ… **State Management** â€” Clean KV cache clearing between messages
- âœ… **Streaming Pipeline** â€” Token-by-token generation with intelligent buffering
- âœ… **Regex Filtering** â€” Catches token variations and edge cases

### Previous Features
- **Landing Page & Model Selector** â€” Intuitive UI for browsing and selecting models
- **Chat Interface** â€” ChatGPT-style conversation view with message history
- **Model Management** â€” Download, caching, and switching capabilities
- **CI/CD Integration** â€” GitHub Actions workflow for automated reviews
- **llama.cpp Submodule** â€” Official llama.cpp integration via git submodule

### Technical Stack
- SwiftUI for modern, declarative UI
- llama.cpp with Metal acceleration for GPU inference
- Swift async/await for streaming token generation
- Actor pattern for thread-safe context management

